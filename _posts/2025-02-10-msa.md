---
layout: post
title: "What is MSA and how do protein LLMs obviate them?"
date: 2025-02-10
mathjax: true
---

MSA - multiple sequence alignment is a method for augmenting a query protein sequence with 'other similar sequences', so that instead of creating features from the single protein sequence, features of the related sequences can be used in addition. This is useful because it provides a richer representation of the query sequence. Intuitively, let's say we wanted to build a machine learning model on people to predict a downstream aspect of each person, say their movie tastes. Then it might help to include the features of each persons friends. AlphaFold2 uses MSA in creating features for a query sequence. 

MSA is a purely statistical procedure. It points a query sequence at a lage database of sequences, and using predefined algorithms (e.g. BLAST), it finds similar sequences (think small edit distance), not necessarily all of the same length. Protein sequences are fully determined by the sequence of DNA base-pairs that generated them, and since DNA mutates across evolutionary time, it is likely that two protein sequences that are very similar are evolutionarilty related. 

Essentially MSA is a way to better _represent_ a protein. But it is not the only way. What if we did _representation learning_ on protein sequences in the same way that we do it for language - words and sentences. BERT is a masked language model where words in sentences are randomly masked and the model tries to predict them from the unmasked words. This ultimately provides a way to _represent_ words and entire sentences, i.e. to _embed_ a word or a sentence into a vector in $$R^n$$ such that two words or sentences in this space are likely to have very similar _meaning_. In the same way, a BERT-style model can be used to build a protein-LLM, treating the amino acids in the protein sequence as tokens in a masked language model, and ultimately being able to represent any protein as an embedding. Since the representation model is built using a very large number of protein sequences for training, it already bakes in information from proteins that might be evolutionarily related to a query protein sequence. Therefore, the MSA step isn't necessary - we already have a rich representation of the protein. 

MSA is a cumbersome procedure and it cannot be made part of an end to end learning architecture. A protein language model serves as a substitute for MSA – it allows us to use the protein embedding produced by the protein LM as the representation of the protein in a downstream task – and, even better, the protein LM can be made part of a longer chained model, and the whole thing optimized/fine tuned end to end -- that is the initial representations of protein from a standalone protein LM can serve as a starting point and then the protein LM architecture plugged in to the downstream architecture to predict a downstream quantity like structure so that the representations are not frozen but are allowed to be fine tuned. This is not possible with MSA.
