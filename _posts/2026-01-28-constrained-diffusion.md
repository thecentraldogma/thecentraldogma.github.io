---
layout: post
title: "Constrained Sampling in Diffusion Models"
date: 2026-01-28
mathjax: true
---

Diffusion models like DDPM (Denoising Diffusion Probabilistic Models) have been quite successful in 2D image generation of course, but also in the protein design community. For example, RFDiffusion is based on the DDPM paradigm. However, DDPM is an unconstrained sampler. That is, after having learnt from a sample of training data, when it is time to generate samples from the model, that will hopefully be from the same distribution as the training data, vanilla DDPM does not impose any constraints on what the generated samples should look like. In practice, many if not most uses of diffusion model based sampling do require some constraints. For example I may want to condition the generation of an image on the text prompt 'cute poodle standing in the rain'. Or I may want a protein backbone that has a specific motif. 

There are 3 general strategies to do conditional or constrained sampling from a DDPM, and they are not exclusive, i.e. they can generally be used in concert with one another: 

- Parameterize the sampling such that all samples always obey a hard constraint. For example, in RFDiffusion, we may need to have a specific motif in the final design. In the original RFDiffusion paper, this is handled by not including such motifs in the forward noising and the backward de-noising at all -- only other parts of the molecule are noised and denoised. 

- Project intermediate samples into the feasible set. This is a bit hard to do because its not always clear what the projection function should be. If an intermediate sample does not satisfy the constraint set, then typically we apply a local repair function that _almost_ fixes the problem, i.e. brings the data point close to the manifold of the feasible set, so that it does not deviate too far. And then we continue sampling. Importantly, we apply the projection operator to intermediate samples, and not just to the final sample, because waiting till the very end to project risks destroying a lot of the structure in the data. The strength of this projection operation is typically increased as we make progress in sampling, i.e. not applied much at high levels of noise. To my knowledge the first paper to propose this form of projection at intermediate steps was: 'Denoising Diffusion Restoration Models (DDRM; Kawar et al., NeurIPS 2022)'.

- Finally, we can _guide_ the donoising process by adding a guidance term to the loss function. The plain vanilla loss function is the L2 loss between the true unit norm noise and the actual unit norm noise that was added, and this causes the subtraction of the predicted noise to move the sampler $$x_{t} \rightarrow x_{t-1}$$ in the direction of the gradient of the log likelihood of the data $$\nabla_x p_{\epsilon}{x_t}$$. To this we add a second term $$\lambda \nabla p_{\epsilon}(E)$$ where $$E$$ is a manually crafted _energy function_ that is differentiable in $$x$$ and has higher values when contraints are violated more severely, and $\lambda$ is a multiplier that controls the strength of this guidance term. $$\lambda$$ is typically increased as sampling progresses. This idea first appears in Diffusion Models Beat GANs on Image Synthesis, Dhariwal and Nichols 2021 where it is framed as classifier based guidance, i.e. instead of an energy term, the authors use $$\log p(y \mid x)$$ so the energy here is -$$\log p(y \mid x)$$. The paper Diffusion Models as Energy-Based Models, Grathwohl et al 2021 reinterpreted classifier based guidance as more general enery-based guidance. While the original RFDiffusion paper does not use an energy-based guidance term at sampling time, others have tried adding this on.
